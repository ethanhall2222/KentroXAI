KentroXAI Repository Guide
Generated: 2026-02-18

1) Executive Summary
KentroXAI is an offline-first Trusted AI governance toolkit implemented as a Python package.
It is organized around four workstreams:
- Workstream A: Explainability and Reasoning Report
- Workstream B: Evaluation and Evidence Pipeline
- Workstream C: Security/Red Teaming, Monitoring, and Incident Response
- Workstream D: Documentation Templates and Artifact Repository Structure

Primary usage pattern:
- Run one command (tat run prompt ...) to generate a full evidence pack under artifacts/<run_id>/.


2) Repository Structure
Root files:
- README.md: Usage and command guide
- pyproject.toml: Packaging and CLI entrypoint
- config.yaml: Runtime configuration generated by tat init

Source package:
- src/trusted_ai_toolkit/
  - cli.py: Main Typer CLI entrypoint and command orchestration
  - schemas.py: Pydantic contracts for config, scorecards, incidents, monitoring, lineage, artifacts
  - config.py: YAML config loader and env-var overrides
  - artifacts.py: Artifact writing helpers and manifest generation
  - documentation.py: Workstream D docs and manifest build pipeline
  - reporting.py: Scorecard logic, stage gates, go/no-go
  - monitoring.py: Telemetry logger and summary aggregation
  - incident.py: Incident trigger and incident record generation
  - eval/
    - runner.py: Evaluation execution engine
    - metrics/__init__.py: Metric registry and deterministic metrics
    - suites/{low,medium,high}.yaml: Golden test suite definitions (54 total cases)
  - redteam/
    - runner.py: Red-team case execution
    - cases/__init__.py: 20 deterministic red-team scenarios with tags
  - xai/
    - reasoning_report.py: Workstream A report generation
    - lineage.py: Lineage report and authoritative source index generation
  - templates/
    - reasoning_report.md.j2
    - scorecard.md.j2
    - scorecard.html.j2
    - incident_template.md.j2
    - system_card.md.j2
    - data_card.md.j2
    - model_card.md.j2
    - lineage_report.md.j2
    - artifact_manifest.md.j2

Tests:
- tests/test_config.py
- tests/test_artifacts.py
- tests/test_eval.py
- tests/test_redteam.py
- tests/test_reporting.py
- tests/test_run_prompt.py


3) How the System Works End-to-End
Command:
- tat run prompt --config config.yaml --prompt "..." [--model-output "..."] [--context-file path.json]

Execution sequence:
1. Creates run folder: artifacts/<run_id>/
2. Writes prompt metadata: prompt_run.json
3. Runs evaluation suites and writes eval_results.json
4. Runs red-team suite and writes redteam_findings.json + redteam_summary.json
5. Generates explainability artifacts:
   - reasoning_report.md + reasoning_report.json
   - lineage_report.md
   - authoritative_data_index.json
6. Generates governance scorecard:
   - scorecard.md + scorecard.html + scorecard.json
7. Summarizes telemetry:
   - monitoring_summary.json
8. Builds docs/templates artifacts:
   - system_card.md
   - data_card.md
   - model_card.md
   - artifact_manifest.json + artifact_manifest.md
9. Auto-generates incident artifacts when thresholds/gates/anomalies require escalation:
   - incident_report.md + incident_report.json
10. Writes telemetry stream:
   - telemetry.jsonl


4) README and Run Instructions (Current)
From README.md:

Quickstart:
- pip install -e .
- tat init
- tat run prompt --config config.yaml --prompt "Summarize the policy update."

Open scorecard HTML:
- open artifacts/$RUN_ID/*.html

Core commands:
- tat run prompt --config config.yaml --prompt "Summarize policy controls" --model-output "Stub answer"
- tat eval run --config config.yaml
- tat redteam run --config config.yaml
- tat xai reasoning-report --config config.yaml
- tat report --config config.yaml
- tat docs build --config config.yaml
- tat monitor summarize --config config.yaml
- tat incident generate --config config.yaml


5) Outputs You Care About Most
Decision artifacts:
- scorecard.md
- scorecard.html
- scorecard.json
- reasoning_report.md
- redteam_findings.json
- monitoring_summary.json
- incident_report.md (when triggered)

Evidence/compliance artifacts:
- eval_results.json
- lineage_report.md
- authoritative_data_index.json
- system_card.md
- data_card.md
- model_card.md
- artifact_manifest.json
- artifact_manifest.md
- telemetry.jsonl
- prompt_run.json


6) Workstream Mapping
Workstream A (Explainability):
- reasoning_report.md/.json
- lineage_report.md
- authoritative_data_index.json

Workstream B (Evaluation + Evidence):
- eval_results.json
- scorecard.md/.html/.json

Workstream C (Security + Ops):
- redteam_findings.json
- redteam_summary.json
- monitoring_summary.json
- incident_report.md/.json

Workstream D (Docs + Repository Artifacts):
- system_card.md
- data_card.md
- model_card.md
- artifact_manifest.json/.md


7) Quality and Validation
- Test suite: pytest
- Current tests cover config, artifact writing/manifest, eval flow, red-team flow, scorecard logic, and orchestration outputs.
- Golden evaluation cases: 54 (low/medium/high suites)
- Red-team deterministic cases: 20


8) Notes and Operational Guidance
- This v0 is offline-first and deterministic by design.
- External provider integration (for example Azure OpenAI) is scaffolded via adapter config but not required.
- High-risk and severe findings intentionally drive stage-gate failures and no-go outcomes until mitigated.


9) Quick Commands to Inspect Latest Run
macOS shell:
- RUN_ID=$(ls -1t artifacts | head -n 1)
- ls -1 artifacts/$RUN_ID
- cat artifacts/$RUN_ID/scorecard.md
- open artifacts/$RUN_ID/scorecard.html
- code artifacts/$RUN_ID/*.md


10) Recommended Cleanup Before Sharing Publicly
Your current repo includes generated artifacts and cache/build outputs. Consider adding .gitignore entries for:
- artifacts/
- __pycache__/
- .pytest_cache/
- *.egg-info/
Then keep one sample run folder separately if needed for demo purposes.
